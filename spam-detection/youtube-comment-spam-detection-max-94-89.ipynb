{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-17T12:01:56.085263Z",
     "iopub.status.busy": "2024-08-17T12:01:56.084342Z",
     "iopub.status.idle": "2024-08-17T12:01:56.543074Z",
     "shell.execute_reply": "2024-08-17T12:01:56.542083Z",
     "shell.execute_reply.started": "2024-08-17T12:01:56.085231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/spam-detection'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:35:58.908340Z",
     "iopub.status.busy": "2024-08-17T12:35:58.907478Z",
     "iopub.status.idle": "2024-08-17T12:35:58.939734Z",
     "shell.execute_reply": "2024-08-17T12:35:58.938795Z",
     "shell.execute_reply.started": "2024-08-17T12:35:58.908296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "COMMENT_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AUTHOR",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CONTENT",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "VIDEO_NAME",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "CLASS",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1c98b10b-7254-4303-a255-a607458af5aa",
       "rows": [
        [
         "0",
         "LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU",
         "Julius NM",
         "2013-11-07T06:20:48",
         "Huh, anyway check out this you[tube] channel: kobyoshi02",
         "PSY - GANGNAM STYLE(?????) M/V",
         "1"
        ],
        [
         "1",
         "LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A",
         "adam riyati",
         "2013-11-07T12:37:15",
         "Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!",
         "PSY - GANGNAM STYLE(?????) M/V",
         "1"
        ],
        [
         "2",
         "LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8",
         "Evgeny Murashkin",
         "2013-11-08T17:34:21",
         "just for test I have to say murdev.com",
         "PSY - GANGNAM STYLE(?????) M/V",
         "1"
        ],
        [
         "3",
         "z13jhp0bxqncu512g22wvzkasxmvvzjaz04",
         "ElNino Melendez",
         "2013-11-09T08:28:43",
         "me shaking my sexy ass on my channel enjoy ^_^ ﻿",
         "PSY - GANGNAM STYLE(?????) M/V",
         "1"
        ],
        [
         "4",
         "z13fwbwp1oujthgqj04chlngpvzmtt3r3dw",
         "GsMega",
         "2013-11-10T16:05:38",
         "watch?v=vtaRGgvGtWQ   Check this out .﻿",
         "PSY - GANGNAM STYLE(?????) M/V",
         "1"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>VIDEO_NAME</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>PSY - GANGNAM STYLE(?????) M/V</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "                       VIDEO_NAME  CLASS  \n",
       "0  PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "1  PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "2  PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "3  PSY - GANGNAM STYLE(?????) M/V      1  \n",
       "4  PSY - GANGNAM STYLE(?????) M/V      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Youtube-Spam-Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:01:56.612137Z",
     "iopub.status.busy": "2024-08-17T12:01:56.611799Z",
     "iopub.status.idle": "2024-08-17T12:01:56.637363Z",
     "shell.execute_reply": "2024-08-17T12:01:56.636389Z",
     "shell.execute_reply.started": "2024-08-17T12:01:56.612109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1956 entries, 0 to 1955\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   COMMENT_ID  1956 non-null   object\n",
      " 1   AUTHOR      1956 non-null   object\n",
      " 2   DATE        1711 non-null   object\n",
      " 3   CONTENT     1956 non-null   object\n",
      " 4   VIDEO_NAME  1956 non-null   object\n",
      " 5   CLASS       1956 non-null   int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 91.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:01:56.640252Z",
     "iopub.status.busy": "2024-08-17T12:01:56.639910Z",
     "iopub.status.idle": "2024-08-17T12:01:56.648870Z",
     "shell.execute_reply": "2024-08-17T12:01:56.647797Z",
     "shell.execute_reply.started": "2024-08-17T12:01:56.640221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f39a8f38-2a87-4f8d-9b32-c34867cee52a",
       "rows": [
        [
         "COMMENT_ID",
         "0"
        ],
        [
         "AUTHOR",
         "0"
        ],
        [
         "DATE",
         "245"
        ],
        [
         "CONTENT",
         "0"
        ],
        [
         "VIDEO_NAME",
         "0"
        ],
        [
         "CLASS",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/plain": [
       "COMMENT_ID      0\n",
       "AUTHOR          0\n",
       "DATE          245\n",
       "CONTENT         0\n",
       "VIDEO_NAME      0\n",
       "CLASS           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:36:04.602635Z",
     "iopub.status.busy": "2024-08-17T12:36:04.601792Z",
     "iopub.status.idle": "2024-08-17T12:36:04.716171Z",
     "shell.execute_reply": "2024-08-17T12:36:04.715250Z",
     "shell.execute_reply.started": "2024-08-17T12:36:04.602604Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       176\n",
      "           1       0.95      0.83      0.89       216\n",
      "\n",
      "    accuracy                           0.89       392\n",
      "   macro avg       0.89      0.89      0.89       392\n",
      "weighted avg       0.89      0.89      0.89       392\n",
      "\n",
      "Accuracy: 0.8852040816326531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "data['CONTENT'] = data['CONTENT'].apply(clean_text)\n",
    "X = data['CONTENT']\n",
    "y = data['CLASS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vect)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:37:08.321427Z",
     "iopub.status.busy": "2024-08-17T12:37:08.321025Z",
     "iopub.status.idle": "2024-08-17T12:37:25.018306Z",
     "shell.execute_reply": "2024-08-17T12:37:25.017195Z",
     "shell.execute_reply.started": "2024-08-17T12:37:08.321394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88       176\n",
      "           1       0.89      0.91      0.90       216\n",
      "\n",
      "    accuracy                           0.89       392\n",
      "   macro avg       0.89      0.89      0.89       392\n",
      "weighted avg       0.89      0.89      0.89       392\n",
      "\n",
      "Accuracy: 0.8903061224489796\n",
      "-----------------------------\n",
      "GradientBoostingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87       176\n",
      "           1       0.96      0.81      0.88       216\n",
      "\n",
      "    accuracy                           0.88       392\n",
      "   macro avg       0.88      0.88      0.87       392\n",
      "weighted avg       0.89      0.88      0.88       392\n",
      "\n",
      "Accuracy: 0.875\n",
      "-----------------------------\n",
      "AdaBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82       176\n",
      "           1       0.94      0.70      0.81       216\n",
      "\n",
      "    accuracy                           0.81       392\n",
      "   macro avg       0.83      0.83      0.81       392\n",
      "weighted avg       0.84      0.81      0.81       392\n",
      "\n",
      "Accuracy: 0.8137755102040817\n",
      "-----------------------------\n",
      "ExtraTreesClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88       176\n",
      "           1       0.90      0.92      0.91       216\n",
      "\n",
      "    accuracy                           0.90       392\n",
      "   macro avg       0.90      0.90      0.90       392\n",
      "weighted avg       0.90      0.90      0.90       392\n",
      "\n",
      "Accuracy: 0.8979591836734694\n",
      "-----------------------------\n",
      "BaggingClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       176\n",
      "           1       0.87      0.89      0.88       216\n",
      "\n",
      "    accuracy                           0.87       392\n",
      "   macro avg       0.87      0.86      0.87       392\n",
      "weighted avg       0.87      0.87      0.87       392\n",
      "\n",
      "Accuracy: 0.8673469387755102\n",
      "-----------------------------\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       176\n",
      "           1       0.94      0.91      0.92       216\n",
      "\n",
      "    accuracy                           0.92       392\n",
      "   macro avg       0.92      0.92      0.92       392\n",
      "weighted avg       0.92      0.92      0.92       392\n",
      "\n",
      "Accuracy: 0.9183673469387755\n",
      "-----------------------------\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87       176\n",
      "           1       0.94      0.82      0.88       216\n",
      "\n",
      "    accuracy                           0.87       392\n",
      "   macro avg       0.88      0.88      0.87       392\n",
      "weighted avg       0.88      0.87      0.87       392\n",
      "\n",
      "Accuracy: 0.8724489795918368\n",
      "-----------------------------\n",
      "CatBoostClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.89       176\n",
      "           1       0.96      0.83      0.89       216\n",
      "\n",
      "    accuracy                           0.89       392\n",
      "   macro avg       0.89      0.90      0.89       392\n",
      "weighted avg       0.90      0.89      0.89       392\n",
      "\n",
      "Accuracy: 0.8903061224489796\n",
      "-----------------------------\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       176\n",
      "           1       0.94      0.82      0.87       216\n",
      "\n",
      "    accuracy                           0.87       392\n",
      "   macro avg       0.87      0.88      0.87       392\n",
      "weighted avg       0.88      0.87      0.87       392\n",
      "\n",
      "Accuracy: 0.8698979591836735\n",
      "-----------------------------\n",
      "MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89       176\n",
      "           1       0.89      0.94      0.92       216\n",
      "\n",
      "    accuracy                           0.91       392\n",
      "   macro avg       0.91      0.90      0.90       392\n",
      "weighted avg       0.91      0.91      0.91       392\n",
      "\n",
      "Accuracy: 0.9056122448979592\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danialmbinmadrawi/miniforge3/envs/translator/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    ExtraTreesClassifier(random_state=42),\n",
    "    BaggingClassifier(random_state=42),\n",
    "    SVC(random_state=42),\n",
    "    XGBClassifier(random_state=42),\n",
    "    CatBoostClassifier(random_state=42, verbose=False),\n",
    "    LGBMClassifier(random_state=42, verbose=-1),\n",
    "    MultinomialNB()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "    print(model.__class__.__name__)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning RFC and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:01:57.485111Z",
     "iopub.status.busy": "2024-08-17T12:01:57.482846Z",
     "iopub.status.idle": "2024-08-17T12:08:14.988033Z",
     "shell.execute_reply": "2024-08-17T12:08:14.986916Z",
     "shell.execute_reply.started": "2024-08-17T12:01:57.485075Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       176\n",
      "           1       0.96      0.85      0.90       216\n",
      "\n",
      "    accuracy                           0.90       392\n",
      "   macro avg       0.90      0.90      0.90       392\n",
      "weighted avg       0.90      0.90      0.90       392\n",
      "\n",
      "Accuracy: 0.8954081632653061\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "grid_search.fit(X_train_vect, y_train)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_rf = best_rf_model.predict(X_test_vect)\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:42:21.485785Z",
     "iopub.status.busy": "2024-08-17T12:42:21.484915Z",
     "iopub.status.idle": "2024-08-17T12:42:40.715383Z",
     "shell.execute_reply": "2024-08-17T12:42:40.714352Z",
     "shell.execute_reply.started": "2024-08-17T12:42:21.485748Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       176\n",
      "           1       0.92      0.92      0.92       216\n",
      "\n",
      "    accuracy                           0.91       392\n",
      "   macro avg       0.91      0.91      0.91       392\n",
      "weighted avg       0.91      0.91      0.91       392\n",
      "\n",
      "Accuracy: 0.9132653061224489\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(random_state=42), param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)\n",
    "grid_search.fit(X_train_vect, y_train)\n",
    "y_pred = grid_search.best_estimator_.predict(X_test_vect)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert (max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:08:14.990125Z",
     "iopub.status.busy": "2024-08-17T12:08:14.989821Z",
     "iopub.status.idle": "2024-08-17T12:11:33.098762Z",
     "shell.execute_reply": "2024-08-17T12:11:33.097669Z",
     "shell.execute_reply.started": "2024-08-17T12:08:14.990097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a01d53b3d8346b79d4b9cc2cc671105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f317b280e9f42e4bbb388e432df63a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdbd5f3e69a4f41be473feb98937376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9066a34bb53c4e7d867ca281e597aa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5110749ccf19480eb72a5ed81f680719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       176\n",
      "           1       0.96      0.95      0.95       216\n",
      "\n",
      "    accuracy                           0.95       392\n",
      "   macro avg       0.95      0.95      0.95       392\n",
      "weighted avg       0.95      0.95      0.95       392\n",
      "\n",
      "Accuracy: 0.9489795918367347\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SpamDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = SpamDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:11:33.101046Z",
     "iopub.status.busy": "2024-08-17T12:11:33.100403Z",
     "iopub.status.idle": "2024-08-17T12:13:13.068746Z",
     "shell.execute_reply": "2024-08-17T12:13:13.067814Z",
     "shell.execute_reply.started": "2024-08-17T12:11:33.101008Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94a798f62c44259a841f99e60c00242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c4a866434f477fa2cbe996798a9558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cee91bd7f024e52914e54e69252626b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffd10925ebc4198baeb6417dfea62fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7230c0e5a3442a19eea9cd77d3e59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59834ecd2ce8458b9d8d5b797ee7abb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       176\n",
      "           1       0.94      0.94      0.94       216\n",
      "\n",
      "    accuracy                           0.94       392\n",
      "   macro avg       0.94      0.94      0.94       392\n",
      "weighted avg       0.94      0.94      0.94       392\n",
      "\n",
      "Accuracy: 0.9362244897959183\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SpamDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = SpamDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:13:13.072223Z",
     "iopub.status.busy": "2024-08-17T12:13:13.071927Z",
     "iopub.status.idle": "2024-08-17T12:14:05.589904Z",
     "shell.execute_reply": "2024-08-17T12:14:05.588981Z",
     "shell.execute_reply.started": "2024-08-17T12:13:13.072198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f39ca17f82a4298bec35a54a4e8bcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3926f53bbc641dc97e270da3c40350d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100029ba9fb1416db3ab4160c19f49a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d0b8235a5b41238fb4c0634dff131c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa8a33c46e04b91a39d66ffc6a10995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       176\n",
      "           1       0.97      0.92      0.94       216\n",
      "\n",
      "    accuracy                           0.94       392\n",
      "   macro avg       0.93      0.94      0.94       392\n",
      "weighted avg       0.94      0.94      0.94       392\n",
      "\n",
      "Accuracy: 0.9362244897959183\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SpamDataset(train_encodings, y_train.tolist())\n",
    "test_dataset = SpamDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-17T12:33:46.756114Z",
     "iopub.status.busy": "2024-08-17T12:33:46.755480Z",
     "iopub.status.idle": "2024-08-17T12:33:56.106957Z",
     "shell.execute_reply": "2024-08-17T12:33:56.105857Z",
     "shell.execute_reply.started": "2024-08-17T12:33:46.756077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.453438115059113\n",
      "Epoch 2/10, Loss: 0.2550297469204786\n",
      "Epoch 3/10, Loss: 0.14371878710784475\n",
      "Epoch 4/10, Loss: 0.0854994481505484\n",
      "Epoch 5/10, Loss: 0.05706209722640259\n",
      "Epoch 6/10, Loss: 0.05832438768666922\n",
      "Epoch 7/10, Loss: 0.051246509440623374\n",
      "Epoch 8/10, Loss: 0.057802473833993534\n",
      "Epoch 9/10, Loss: 0.03890858628616041\n",
      "Epoch 10/10, Loss: 0.035322406351784894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       176\n",
      "           1       0.94      0.92      0.93       216\n",
      "\n",
      "    accuracy                           0.92       392\n",
      "   macro avg       0.92      0.92      0.92       392\n",
      "weighted avg       0.92      0.92      0.92       392\n",
      "\n",
      "Accuracy: 0.9209183673469388\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data['CONTENT'])\n",
    "X = tokenizer.texts_to_sequences(data['CONTENT'])\n",
    "X = pad_sequences(X, maxlen=100)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['CLASS'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "train_dataset = SpamDataset(X_train, y_train)\n",
    "test_dataset = SpamDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=5000, embedding_dim=128)\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=(3, 128), stride=1, padding=(2, 0))\n",
    "        self.conv2 = nn.Conv2d(1, 128, kernel_size=(4, 128), stride=1, padding=(3, 0))\n",
    "        self.conv3 = nn.Conv2d(1, 128, kernel_size=(5, 128), stride=1, padding=(4, 0))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128 * 3, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x1 = torch.relu(self.conv1(x)).squeeze(3)\n",
    "        x1 = torch.max_pool1d(x1, kernel_size=x1.size(2)).squeeze(2)\n",
    "        x2 = torch.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = torch.max_pool1d(x2, kernel_size=x2.size(2)).squeeze(2)\n",
    "        x3 = torch.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = torch.max_pool1d(x3, kernel_size=x3.size(2)).squeeze(2)\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
